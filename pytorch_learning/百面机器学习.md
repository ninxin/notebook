### 特征工程

###### 归一化

消除数据之间量纲的影响

1 线性函数归一化，减最小值除极差

2 零均值归一化

使得梯度下降更快收敛

决策树不需要



图像数据不足：

迁移学习，GAN，图像增强



### 模型评估

准确率acc：分类正确的样本占总样本的比例

正负样本不均衡时，占比大的类别是影响它的主要因素

可以用不同类别准确率的平均值



精确率pre：分类正确的正样本个数占分类器判定为正样本的个数

召回率Recall：分类正确的正样本个数占真正正样本的个数

为提高pre，需要更有把握时才预测为正样本，这样会过于保守，使得recall降低



F1是pre和recall的调和平均值

ROC：横坐标假阳性率FPR：FP/N

纵轴真阳性率TPR： TP/P

P为正样本的数量。N为负样本的数量，TP是P个正样本被预测为正样本的数量，FP为N个负样本中预测为正样本的数量

相比于P-R曲线，ROC不易受正负样本分布变化的影响。



###### 超参数优化

网格搜索：查找搜索范围所有的点来确定最优值，十分耗费时间和计算资源

一般会先用较广的搜索范围和步长来寻找最优值的可能位置，再缩小范围和步长来找到最优值



随即搜索：在搜索范围中随机选取，速度更快，但结果无法保证



贝叶斯优化算法：会利用之前的信息，来对目标函数形状学习。

更具经验分布假设一个函数，然后用新的采样点来测试，利用这个信息来更新目标函数的分布，再由后验分布来给出最优值最有可能的位置。

容易陷入局部最优



过拟合：模型在训练时表现好，测试表现不好

欠拟合：表现都不好

**降低过拟合**：

1. 使用更多数据，图像增强，生成对抗网络来合成大量新的训练数据
2. 降低模型复杂度，减少网络层数，神经元个数
3. 正则化方法
4. 集成学习，利用多个模型来降低过拟合的风险



### 经典算法：

###### 支持向量机

找到一个超平面分开不同的类别

如果不是线性可分，使用核映射，映射到更高维的空间



###### 决策树

根据不同的特征和属性生成一颗树形的分类结构

ID3：最大信息增益

计算集合的经验熵

计算不同特征对集合的经验条件熵

相减就是信息增益

最大的为根节点



C4.5： 最大信息增益比

CART: 最大基尼指数

只有CART既可分类又能回归



预剪枝：在生成树的过程中

判断当前的划分能否带来泛化能力的提升

算法简单，思想直接，效率高，适合解决大规模问题，要一定的经验

后剪枝：在生成的树上剪枝

从底层向上计算是否剪枝，也可通过在测试集上的准确率是否提升来判断

通常可以得到泛化能力更强的决策树，但时间慢



### 优化算法

不同的损失函数优化难度不同，得到的模型参数也不同

###### 损失函数

二分类：

0-1损失，非凸非光滑，难以优化

Hing损失，是0-1函数相对紧的凸上界，在=1处不可导，只能用次梯度下降

Logistic损失，是0-1损失函数的凸上界，平滑

交叉熵损失，也是光滑凸上界



回归：

平方损失

绝对损失

Huber损失



梯度下降：每次更新权值要计算所有样本，速度慢

随机梯度下降：每次取一个样本来计算梯度，更新权值，容易受噪声影响，难收敛

批量梯度下降：取一些样本来更新

梯度震荡的方向不停的震荡，平缓的地方更新缓慢

下次的权值=现在的权值 - 梯度 * 步长



动量法:利用累积的历史梯度信息更新梯度（梯度变化了，梯度=通过样本计算的梯度+以前的梯度）

可以通过局部最优点和鞍点



自适应梯度法：通过改变步长

减小震荡方向的步长，增加平缓方向的步长

除以历史梯度的平方

AdaGrad

累加的时间过长时，步长会变得非常小



RMSProp：自适应梯度方法

历史梯度的衰减值



Adam：同时使用



